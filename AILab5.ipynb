{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip install sklearn"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "source": [
    "from sklearn.model_selection import train_test_split\r\n",
    "from datetime import datetime\r\n",
    "from keras.layers import Dense\r\n",
    "from keras.models import Sequential\r\n",
    "import tensorflow as tf\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "from keras.utils.np_utils import to_categorical  "
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "source": [
    "def getXY(filename):\r\n",
    "    training_data_filename = \"occupancy_data/datatraining.csv\"\r\n",
    "    data = pd.read_csv(training_data_filename)\r\n",
    "    data = data.drop(labels=['id'], axis=1)\r\n",
    "    data['date'] = [datetime.strptime(d, \"%Y-%m-%d %H:%M:%S\").timestamp() for d in data['date']] #change to date to seconds\r\n",
    "    X = data.drop(labels=[\"Occupancy\"], axis=1)\r\n",
    "    Y = data['Occupancy']\r\n",
    "    Y = to_categorical(Y,num_classes=2) #convert labels into one hot encoded arrays\r\n",
    "    #normalize the dataset \r\n",
    "    X['date'] = tf.keras.utils.normalize(np.array(X['date']))[0]\r\n",
    "    X['Temperature'] = tf.keras.utils.normalize(np.array(X['Temperature']))[0]\r\n",
    "    X['Humidity'] = tf.keras.utils.normalize(np.array(X['Humidity']))[0]\r\n",
    "    X['Light'] = tf.keras.utils.normalize(np.array(X['Light']))[0]\r\n",
    "    X['CO2'] = tf.keras.utils.normalize(np.array(X['CO2']))[0]\r\n",
    "    X['HumidityRatio'] = tf.keras.utils.normalize(np.array(X['HumidityRatio']))[0]\r\n",
    "    return X, Y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "training_data_filename = \"occupancy_data/datatraining.csv\"\n",
    "X, Y = getXY(training_data_filename)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "source": [
    "#split the dataset into training and validation\r\n",
    "x_train, x_val, y_train, y_val = train_test_split(X, Y, test_size=0.05, random_state=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "source": [
    "x_train.shape, x_val.shape, y_train.shape, y_val.shape #training and validation data shapes"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((7735, 6), (408, 6), (7735, 2), (408, 2))"
      ]
     },
     "metadata": {},
     "execution_count": 101
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "source": [
    "x_train, y_train #training data final output"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(          date  Temperature  Humidity     Light       CO2  HumidityRatio\n",
       " 4355  0.011082     0.010892  0.009558  0.000000  0.007089       0.009346\n",
       " 7447  0.011083     0.011004  0.014084  0.000000  0.010341       0.013988\n",
       " 487   0.011080     0.011300  0.010547  0.000000  0.007097       0.010817\n",
       " 771   0.011080     0.011187  0.009661  0.000000  0.007284       0.009775\n",
       " 779   0.011080     0.011136  0.009617  0.000000  0.007365       0.009673\n",
       " ...        ...          ...       ...       ...       ...            ...\n",
       " 4931  0.011082     0.010409  0.013137  0.000000  0.007024       0.012167\n",
       " 3264  0.011081     0.010736  0.007893  0.000000  0.007056       0.007572\n",
       " 1653  0.011081     0.011214  0.008421  0.000000  0.007619       0.008540\n",
       " 2607  0.011081     0.011428  0.008295  0.021339  0.010706       0.008622\n",
       " 2732  0.011081     0.011863  0.009196  0.024201  0.014827       0.010052\n",
       " \n",
       " [7735 rows x 6 columns],\n",
       " array([[1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        ...,\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.]], dtype=float32))"
      ]
     },
     "metadata": {},
     "execution_count": 102
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "source": [
    "#creating the model\r\n",
    "model = Sequential()\r\n",
    "#hidden layer1\r\n",
    "model.add(Dense(128, activation='relu', input_dim=6))\r\n",
    "#output layer\r\n",
    "model.add(Dense(2, activation='softmax'))\r\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "source": [
    "model.summary() #output the model structure"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 128)               896       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 1,154\n",
      "Trainable params: 1,154\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "source": [
    "model.fit(x_train, y_train, epochs=10) #train the model"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "242/242 [==============================] - 0s 471us/step - loss: 0.5238 - accuracy: 0.7880\n",
      "Epoch 2/10\n",
      "242/242 [==============================] - 0s 475us/step - loss: 0.4298 - accuracy: 0.7878\n",
      "Epoch 3/10\n",
      "242/242 [==============================] - 0s 479us/step - loss: 0.3323 - accuracy: 0.8133\n",
      "Epoch 4/10\n",
      "242/242 [==============================] - 0s 467us/step - loss: 0.2366 - accuracy: 0.9161\n",
      "Epoch 5/10\n",
      "242/242 [==============================] - 0s 471us/step - loss: 0.1695 - accuracy: 0.9688\n",
      "Epoch 6/10\n",
      "242/242 [==============================] - 0s 504us/step - loss: 0.1279 - accuracy: 0.9802\n",
      "Epoch 7/10\n",
      "242/242 [==============================] - 0s 484us/step - loss: 0.1044 - accuracy: 0.9850\n",
      "Epoch 8/10\n",
      "242/242 [==============================] - 0s 475us/step - loss: 0.0904 - accuracy: 0.9867\n",
      "Epoch 9/10\n",
      "242/242 [==============================] - 0s 471us/step - loss: 0.0820 - accuracy: 0.9872\n",
      "Epoch 10/10\n",
      "242/242 [==============================] - 0s 475us/step - loss: 0.0772 - accuracy: 0.9872\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x17cc6348e20>"
      ]
     },
     "metadata": {},
     "execution_count": 105
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "source": [
    "model.evaluate(x_val, y_val) #output how the model is performing with the validation dataset"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "13/13 [==============================] - 0s 385us/step - loss: 0.0927 - accuracy: 0.9853\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.09265478700399399, 0.9852941036224365]"
      ]
     },
     "metadata": {},
     "execution_count": 107
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "source": [
    "test1_filename = \"occupancy_data/datatest.csv\"\r\n",
    "test2_filename = \"occupancy_data/datatest2.csv\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "source": [
    "X_test1, Y_test1 = getXY(test1_filename)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "source": [
    "#testing the model performance with datatest.csv\r\n",
    "model.evaluate(X_test1, Y_test1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "255/255 [==============================] - 0s 349us/step - loss: 0.0766 - accuracy: 0.9883\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.07661452889442444, 0.988333523273468]"
      ]
     },
     "metadata": {},
     "execution_count": 111
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "source": [
    "X_test2, Y_test2 = getXY(test2_filename)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "source": [
    "#testing the model performance with datatest2.csv\r\n",
    "model.evaluate(X_test2, Y_test2)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "255/255 [==============================] - 0s 357us/step - loss: 0.0766 - accuracy: 0.9883\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.07661452889442444, 0.988333523273468]"
      ]
     },
     "metadata": {},
     "execution_count": 114
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#I got same level of accuracy with my keras model and my Neural Network model. I was experiencing overfitting initially\r\n",
    "#but tweaking the parameters got rid of that problem. I think maybe the dataset is not too big or that complex to lear so my Neural\r\n",
    "#Network model didn't have any trouble fitting to it. But I can definitely see that this couldn't work that well with large datasets\r\n",
    "#where I would have multiple classes to clasify and numerous parameters to consider. I think the main problem that could restrict or \r\n",
    "#constrain my neural network model is of overfitting due to the lack of any dropout layers, and also the abscense of other \r\n",
    "#kinds of optimizers and error functions"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}